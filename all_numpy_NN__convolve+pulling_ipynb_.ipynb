{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "def convolve(image, kernel):\n",
        "  # print(image[:10], image.shape)\n",
        "  image = image.reshape(28, 28)\n",
        "  # print(image)\n",
        "  img_h = img_w = len(image)\n",
        "  ker_h, ker_w = kernel.shape\n",
        "  out_img_h = img_h - ker_h + 1\n",
        "  out_img_w = img_w - ker_w + 1\n",
        "  output = np.zeros((out_img_h, out_img_w))\n",
        "\n",
        "  for y in range(out_img_h):\n",
        "    for x in range(out_img_w):\n",
        "      window = image[y:y + ker_h, x:x + ker_w]\n",
        "      output[y, x] = np.sum(window * kernel)\n",
        "  return output\n",
        "\n",
        "\n",
        "def max_pooling(image, kernel_size=2, stride=2):\n",
        "  height, width = image.shape\n",
        "  new_height = (height - kernel_size) // stride + 1\n",
        "  new_width = (width - kernel_size) // stride + 1\n",
        "  pooled = np.zeros((new_height, new_width))\n",
        "\n",
        "  for y in range(0, height - kernel_size + 1, stride):\n",
        "      for x in range(0, width - kernel_size + 1, stride):\n",
        "          window = image[y:y+kernel_size, x:x+kernel_size]\n",
        "          pooled[y // stride, x // stride] = np.max(window)\n",
        "\n",
        "  return pooled\n",
        "\n",
        "\n",
        "\n",
        "def apply_convolution_and_pooling(df):\n",
        "  kernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
        "  pooling_kernel_size = 2\n",
        "  pooling_stride = 2\n",
        "  processed_images = []\n",
        "  for image in range(len(df)):\n",
        "      convolved = convolve(df[image], kernel)\n",
        "      pooled = max_pooling(convolved, pooling_kernel_size, pooling_stride)\n",
        "      processed_images.append(pooled)\n",
        "      if not image % 1000:\n",
        "        print(f'Image: {image} done')\n",
        "\n",
        "  return processed_images"
      ],
      "metadata": {
        "id": "FmGprb_EgQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/mnist_train_small.csv')[:5000]\n",
        "\n",
        "# data.columns.tolist()\n",
        "data['6'] = np.where(data['6'] == 5, 1, 0) # binary classification 5 or not 5\n",
        "\n",
        "data = np.array(data)\n",
        "\n",
        "\n",
        "X = data[:, 1:]\n",
        "# print(X.shape, X[0].shape)\n",
        "y = data[:, 0]\n",
        "\n",
        "processed = apply_convolution_and_pooling(X)\n",
        "\n",
        "X = np.array([image.flatten() for image in processed])\n",
        "\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZaMMqjFJc6_",
        "outputId": "8c59042e-9cea-4820-9048-1b322b598e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 0 done\n",
            "Image: 1000 done\n",
            "Image: 2000 done\n",
            "Image: 3000 done\n",
            "Image: 4000 done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 169)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n"
      ],
      "metadata": {
        "id": "TY9rQRjqEi_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities\n",
        "\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
      ],
      "metadata": {
        "id": "fRWyg8XwEvhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Optimizer_SGD:\n",
        "  def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "\n",
        "  def update_params(self, layer):\n",
        "\n",
        "    if self.momentum:\n",
        "      if not hasattr(layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "      weight_updates = \\\n",
        "        self.momentum * layer.weight_momentums - \\\n",
        "        self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      bias_updates = \\\n",
        "        self.momentum * layer.bias_momentums - \\\n",
        "        self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    else:\n",
        "      weight_updates = -self.current_learning_rate * layer.dweights\n",
        "      bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "\n",
        "  def post_update_params(self):\n",
        "      self.iterations += 1\n"
      ],
      "metadata": {
        "id": "DI4IHog5EwXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YcAMG9Kbc4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Loss:\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    if len(y_true.shape) == 1:\n",
        "        correct_confidences = y_pred_clipped[\n",
        "            range(samples),\n",
        "            y_true\n",
        "        ]\n",
        "    elif len(y_true.shape) == 2:\n",
        "        correct_confidences = np.sum(\n",
        "            y_pred_clipped * y_true,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "    if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "    self.dinputs = -y_true / dvalues\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "  def __init__(self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "  def forward(self, inputs, y_true):\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    if len(y_true.shape) == 2:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[range(samples), y_true] -= 1\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dense1 = Layer_Dense(169, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(64, 2)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(learning_rate=0.05, decay=5e-7)\n",
        "\n",
        "for epoch in range(31):\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "  predictions = np.argmax(loss_activation.output, axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "      y = np.argmax(y, axis=1)\n",
        "  accuracy = np.mean(predictions==y)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "      print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "\n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITUlzNx39BI8",
        "outputId": "8daece1d-096a-4cc8-9a17-5625843f9e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.567, loss: 0.881, lr: 0.05\n",
            "epoch: 10, acc: 0.913, loss: 0.551, lr: 0.0499997750010125\n",
            "epoch: 20, acc: 0.913, loss: 0.496, lr: 0.04999952500451246\n",
            "epoch: 30, acc: 0.913, loss: 0.445, lr: 0.04999927501051235\n"
          ]
        }
      ]
    }
  ]
}